Papers:
- Luo etal 2014: An Extension of BLANC to System Mentions. ACL2014
  (applies to co-reference evaluation)
- Hovy etal 2014: Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation. ACL2104
  (applies to co-reference evaluation)
- Davison&Hinkley 1997: Bootstap methods and their applications. No download/ebook -> Library!
- Lin 2004: Looking for a few good metrics: ROUGE and its evaluation
- Noreen 1989: Computer intensive methods for testing hypotheses -> library
- Tjong Kim Sang & De Meulder 2003: Introduction to the CoNLL-2003 shared task: Language independent named entity recognition.

Slides:
- TAC2014 Entity Linking Scorer: http://nlp.cs.rpi.edu/kbp/2014/edlscorer-apr22.pptx

Other Software:
- TAC2014 measures: https://github.com/wikilinks/neleval/wiki/Evaluation and 
  https://github.com/wikilinks/neleval/wiki/Significance
- TAC EL evaluation and analysis scripts: https://github.com/wikilinks/neleval
- https://code.google.com/p/reference-coreference-scorers/ (coreference evaluation, perl)


Notes about parameters and functionality:

= name/title: to make it easier to identify an evaluation when we run several in a one pipeline
  maybe also: description
= target set name
= response set name
= types and features: a list of type+features specifications. For each type
  the comparison will be made using the given features (if any). In addition,
  an overal micro average will be made and over all types, a macro average. 
= response diff set name: the name of the set that contains annotations which 
  identify missing/spurious etc. 
= reference set name: if given then everything is also done for the pair  
  reference set / target set and the differences between the two are 
  calculated (e.g. 23 more missing).
= referemce diff set name: tje mame of the set that contains annotations which
  represent missing/spurious etc for the reference set
= response vs. reference set name: a set that has annotations the represent
  changes between the response and reference sets with regard to the target,
  This uses the reference set as the "starting point" and creates annotations 
  which represent how the starting point has changed, e.g. was correct now missing. 
= Zero to N "by" features: if this is given than each evaluation will be further
  broken down by each of the values of that feature. 

Calculate P/R/F versus accuracy:
= Accuracy: we count by target, and for each target there is either a correct
  or an incorrect response. We have the following possible situations:
  - there is exactly one response and it matches: correct response
  - there is exactly one response and it does not match: incorrect response
  - there is more than one response: incorrect (also counted separately)
  - there is no response: incorrect (also counted separately) 
  - responses elsewhere are ignored for the accuracy (but counted)
  - we calculate numbers for exact matches and overlaps:
    - for exact matches, only other exact matches are considered 
    - if there is a response that overlaps and another that is strict, the 
      strict one is used for strict accuracy but the lenient accuracy is 
      counted as false (because two overlaps exist). 
  - Q: what if we have coextensive/overlapping targets? 
= P/R/F: we process by target, for each target with find the best coextensive
  or overlapping match.
  - if there is a coextensive match then we count strict correct and use up the match
  - else if there is an overlappign match then we count lenient correct and strict missing and use up the match
  - else if no correct match has been found and we have a coextensive response: count as strict incorrect and strict spurious and strict missing and use up
  - if not correct or coextensive incorrect match has been found and we have an
    overlapping response: count as lenient incorrect and spurious and use up
  - all remaining responses are spurious. 
  
  

Initial development notes.

*) allow to compare two completely different set/type/feature triples. Or rather,
  two sets of such triples, where each pair corresponds to one analysis which could the
  get macro or micro averaged.
*) consider parallel computation on large corpora
*) consider storing intermediate results in document features, then calculating the final numbers
*) consider evaluation on parallel corpora, instead of annotation sets or have a separate
  PR for creating a corpus with parallel sets from parallel documents. This is related to the
  idea of having GATE "documents" which only reference the document and really only consist of
  annotation sets.
  For this we need to define what makes corpora "parallel" and how the unique id of documents
  are established: name (file name, file path portion, stored document name) or id (index number
  in corpus, md5 content hash - not so good because we could have separate documents with the 
  same content?)
  This is also related to the way we model input sources: if we have the concept of parallel
  streams, we do not really have to care and what parallel means is up to the implementation
  for the input source.
*) consider using arbitrary loss functions instead of 0-1 loss for the numbers
*) consider using a different scoring function for the alignment decision. For alignment 
  allow approaches where we limit to coextensive/overalpping for strict and lenient and where
  we consider all pairs and use the scoring function?

Literature, maybe relevant:
  = Mallet pages:
    http://mallet.cs.umass.edu/api/cc/mallet/classify/Trial.html
    http://mallet.cs.umass.edu/api/cc/mallet/classify/evaluate/ConfusionMatrix.html
  = LingPipe:
    http://alias-i.com/lingpipe/docs/api/com/aliasi/tag/TaggerEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredPrecisionRecallEvaluation.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredClassification.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/PrecisionRecallEvaluation.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/JointClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ConditionalClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/RankedClassification.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/RankedClassifierEvaluator.html
  = http://demonstrations.wolfram.com/ThePrecisionRecallCurveInInformationRetrievalEvaluation/
  = http://www.nr.com/CS395T/lectures2008/17-ROCPrecisionRecall.pdf
  = http://clair.eecs.umich.edu/aan/paper.php?paper_id=M98-1030
  = http://www.itl.nist.gov/iaui/894.02/related_projects/muc/muc_sw/muc_sw_manual.html
  = http://en.wikipedia.org/wiki/Named-entity_recognition
  = http://www.cicling.org/2009/RCS-41/047-058.pdf
  = http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/PrecisionRecallEvaluation.html
  = http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredPrecisionRecallEvaluation.html
  = http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html
  = http://www.itl.nist.gov/iad/mig/publications/proceedings/darpa99/html/dir10/dir10.htm
  = http://en.wikipedia.org/wiki/Sensitivity_and_specificity
  = http://repository.dlsi.ua.es/251/1/pdf/869_paper.pdf
    bibliography?
  = http://www.researchgate.net/publication/8048485_Agreement_the_f-measure_and_reliability_in_information_retrieval
  = http://nlp.stanford.edu/IR-book/pdf/08eval.pdf
  = http://www.cnts.ua.ac.be/~vincent/pdf/microaverage.pdf
  = http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1090460/
  = http://blogs.sas.com/content/iml/2011/07/29/computing-an-roc-curve-from-basic-principles/
  = http://gim.unmc.edu/dxtests/ROC1.htm
  = http://www.r-bloggers.com/roc-curves-and-classification/
  = http://weka.wikispaces.com/ROC+curves
  = http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0092209
  = http://www.vlfeat.org/overview/plots-rank.html
  = https://www.creighton.edu/fileadmin/user/HSL/docs/ref/Searching_-_Recall_Precision.pdf
  = http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html
  = http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html
  = http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_DavisG06.pdf
  = http://en.wikipedia.org/wiki/Precision_and_recall
 

MISC NOTES:
= algorithm for minimum cost assignment, useful for minimum loss coreference cluster 
  assignments: Munkres/Hungarian algorithm, see http://en.wikipedia.org/wiki/Hungarian_algorithm
  for perl: https://metacpan.org/pod/Algorithm::Munkres 
  explanation and c code: http://csclab.murraystate.edu/bob.pilgrim/445/munkres.html
  explanation/tutorial: http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=hungarianAlgorithm
  implementation/java: https://github.com/KevinStern/software-and-algorithms/blob/master/src/main/java/blogspot/software_and_algorithms/stern_library/optimization/HungarianAlgorithm.java
  implementation/java: https://code.google.com/p/robotutils/source/browse/trunk/src/robotutils/planning/KuhnMunkres.java
