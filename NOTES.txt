DEFINITIONS:
============

For P/R/F:
----------

An annotation is either consumed or not-consumed: if we found some response
annotation that was the best match for that target then both the target and response
annotations are consumed. A not-consumed annotation is an available annotation. 

In the following, if the response annotation is compared to a target annotation,
then both the target and response annotations must be uncomsumed and consumed
by the match, i.e. neither the target nor the response are used for any other match. 

A correctStrict annotation is a response  that is coextensive and equal.
A correctPartial annotation is a response that is overlapping but not coexgensive and equal.
A correctLenient annotation is one that is overlapping and equal. The set of 
  these annotations is the union of correctStrict and correctPartial.

An incorrectStrict annotation is a response that is coextensive and not equal.
An incorrectPartial annotation is one that is overlapping but not coextensive and not equal.
An incorrectLenient annotation is one that is overlapping but tno equal. The set of 
  these is the union of incorrectStrict and incorrectPartial

A missing annotation is a target anotation for which no correct response annotation
exists. 
A missingStrict annotation is a target annotation for which no correctStrict response exists.
A missingLenient annotation is a target annotation for which no correctStrict or correctPartial response
exists. 
[missingPartial does not make sense]

A spurious annotation is a response annotation for which no correct target annotation exists.
A spuriousStrict is a response annotation which is not correctStrict
A spuriousLenient is a response annotation which is not correctLenient
[spuriousPartial does not make sense]

A "trueMissing" annotation is a key annotation for which there is not even an
incorrect response (and obviously no correct response either).
A trueMissingStrict annotation is a key annotation for which there is no 
correctStrict and no incorrectStrict annotation
A trueMissingLenient annotation is a key annotation for which there is no 
correctLenient and no incorrectLenient annotation.

A "trueSpurious" annotation is a response annotation which is not 
incorrect or correct.  
A trueSpuriousStrict annotation is a response annotation which is neither correctStrict 
nor incorrectStrict.
A trueSpuriousLenient annotation is a response annotation which is neither 
correctLenient nor incorrectLenient. 

The following should hold

responses = correctStrict + spuriousStrict
responses = correctLenient + spuriousLenient
responses = correctStrict + incorrectStrict + trueSpuriousStrict
responses = correctLenient + incorrectLenient + trueSpuriousLenient

targets = correctStrict + missingStrict
targets = correctLenient + missingLenient
targets = correctStrict + incorrectStrict + trueMissingStrict
targets = correctLenient + incorrectLenient + trueMissingLenient


Precision measures the proportion of correct responses among all responses. 
precisionStrict = correctStrict/responses
precisionLenient = correctLenient/responses

Recall measures the proportion of correct responses amont all targets.
recallStrict =  correctStrict / targets
recallLenient = correctLenient / targets 

Error rate measures the proportion of incorrect responses among all responses
errorRateStrict = incorrectStrict/responses
errorRateLenient = incorrectLenient/responses

FalsePositives (as in AnnDiff) is the number of all responses that are not 
correct:
n_falsePositiveStrict = responses - n_correctStrict
n_falsePositiveLenient = responses - n_correctStrict - n_correctPartial = responses - n_correctLenient


Strategy for finding the counts:
--------------------------------

initialize the sets missingStrict, missingPartial, trueMissingStrict and trueMissingPartial
with all the targets. 

For each response we have one of the following situations:
= if we use the score and there is a new score, add a new stat object for that score
= there is no overlapping target: this is a trueSpuriousStrict response and a trueSpuriousLenient response
= there are overlappign targets:
  go through all overlapping targets and assign the first target which matches
  the conditions below, then after all targets have been processed, check 
  which is the first which we can process:
  - the response strictly matches and equals one or more of the targets
    pick one it matches and remove from available targets, count as correctStrict
    remove from missingStrict
    remove from trueMissingStrict
  - the response leniently matches and equals one or more of the targets
    pick one it matches (maybe longest overlap!) and remove from available targets, count as correctLenient
    remove from missingPartial
    remove from trueMissingPartial
  - the response strictly matches and does not equal one or more targets
    pick one it is coextensive with and remove from available targets, count as incorrectStrict
    remove from trueMissingStrict
  - the response leniently matches and does not equal one ore more targets
    pick one (maybe the one with longest overlap) and remove from targets, count as incorrectLenient
    remove from trueMissingPartial
  - no other options exist

  
  
  
  
Note: is it faster to just process for each response all targets (resonse x targets comparisons, no 
building of additional data structures) or to find the overlapping ones for each response
(on average much less than |targets| per response, but we need to find the overlaps first which
may be much more inefficient.
  
  
  
if a response has score s then we also have to count the response individually
for all the stats with thresholds t_i

if s < t_i we ignore the response, it is the same as if it was not here. 
if s >= t_i then we count the response as above for the stat obj with threshold t_i
  
  
=====================





Papers:
- !!! Cyril Goutte and Eric Gaussier: A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation
  (Goutte2005)

- Luo etal 2014: An Extension of BLANC to System Mentions. ACL2014
  (applies to co-reference evaluation)
- Hovy etal 2014: Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation. ACL2104
  (applies to co-reference evaluation)
- Davison&Hinkley 1997: Bootstap methods and their applications. No download/ebook -> Library!
- Lin 2004: Looking for a few good metrics: ROUGE and its evaluation
- Noreen 1989: Computer intensive methods for testing hypotheses -> library
- Tjong Kim Sang & De Meulder 2003: Introduction to the CoNLL-2003 shared task: Language independent named entity recognition.

Slides:
- TAC2014 Entity Linking Scorer: http://nlp.cs.rpi.edu/kbp/2014/edlscorer-apr22.pptx

Other Software:
- TAC2014 measures: https://github.com/wikilinks/neleval/wiki/Evaluation and 
  https://github.com/wikilinks/neleval/wiki/Significance
- TAC EL evaluation and analysis scripts: https://github.com/wikilinks/neleval
- https://code.google.com/p/reference-coreference-scorers/ (coreference evaluation, perl)


Notes about parameters and functionality:

= name/title: to make it easier to identify an evaluation when we run several in a one pipeline
  maybe also: description
= target set name
= response set name
= types and features: a list of type+features specifications. For each type
  the comparison will be made using the given features (if any). In addition,
  an overal micro average will be made and over all types, a macro average. 
= response diff set name: the name of the set that contains annotations which 
  identify missing/spurious etc. 
= reference set name: if given then everything is also done for the pair  
  reference set / target set and the differences between the two are 
  calculated (e.g. 23 more missing).
= referemce diff set name: tje mame of the set that contains annotations which
  represent missing/spurious etc for the reference set
= response vs. reference set name: a set that has annotations the represent
  changes between the response and reference sets with regard to the target,
  This uses the reference set as the "starting point" and creates annotations 
  which represent how the starting point has changed, e.g. was correct now missing. 
= Zero to N "by" features: if this is given than each evaluation will be further
  broken down by each of the values of that feature. 

Calculate P/R/F versus accuracy:
= Accuracy: we count by target, and for each target there is either a correct
  or an incorrect response. We have the following possible situations:
  - there is exactly one response and it matches: correct response
  - there is exactly one response and it does not match: incorrect response
  - there is more than one response: incorrect (also counted separately)
  - there is no response: incorrect (also counted separately) 
  - responses elsewhere are ignored for the accuracy (but counted)
  - we calculate numbers for exact matches and overlaps:
    - for exact matches, only other exact matches are considered 
    - if there is a response that overlaps and another that is strict, the 
      strict one is used for strict accuracy but the lenient accuracy is 
      counted as false (because two overlaps exist). 
  - Q: what if we have coextensive/overlapping targets? 
= P/R/F: we process by target, for each target with find the best coextensive
  or overlapping match.
  - if there is a coextensive match then we count strict correct and use up the match
  - else if there is an overlappign match then we count lenient correct and strict missing and use up the match
  - else if no correct match has been found and we have a coextensive response: count as strict incorrect and strict spurious and strict missing and use up
  - if not correct or coextensive incorrect match has been found and we have an
    overlapping response: count as lenient incorrect and spurious and use up
  - all remaining responses are spurious. 
  
  

Initial development notes.

*) allow to compare two completely different set/type/feature triples. Or rather,
  two sets of such triples, where each pair corresponds to one analysis which could the
  get macro or micro averaged.
*) consider parallel computation on large corpora
*) consider storing intermediate results in document features, then calculating the final numbers
*) consider evaluation on parallel corpora, instead of annotation sets or have a separate
  PR for creating a corpus with parallel sets from parallel documents. This is related to the
  idea of having GATE "documents" which only reference the document and really only consist of
  annotation sets.
  For this we need to define what makes corpora "parallel" and how the unique id of documents
  are established: name (file name, file path portion, stored document name) or id (index number
  in corpus, md5 content hash - not so good because we could have separate documents with the 
  same content?)
  This is also related to the way we model input sources: if we have the concept of parallel
  streams, we do not really have to care and what parallel means is up to the implementation
  for the input source.
*) consider using arbitrary loss functions instead of 0-1 loss for the numbers
*) consider using a different scoring function for the alignment decision. For alignment 
  allow approaches where we limit to coextensive/overalpping for strict and lenient and where
  we consider all pairs and use the scoring function?

Literature, maybe relevant:
  = Mallet pages:
    http://mallet.cs.umass.edu/api/cc/mallet/classify/Trial.html
    http://mallet.cs.umass.edu/api/cc/mallet/classify/evaluate/ConfusionMatrix.html
  = LingPipe:
    http://alias-i.com/lingpipe/docs/api/com/aliasi/tag/TaggerEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredPrecisionRecallEvaluation.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredClassification.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/PrecisionRecallEvaluation.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/JointClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ConditionalClassifierEvaluator.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/RankedClassification.html
    http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/RankedClassifierEvaluator.html
  = http://demonstrations.wolfram.com/ThePrecisionRecallCurveInInformationRetrievalEvaluation/
  = http://www.nr.com/CS395T/lectures2008/17-ROCPrecisionRecall.pdf
  = http://clair.eecs.umich.edu/aan/paper.php?paper_id=M98-1030
  = http://www.itl.nist.gov/iaui/894.02/related_projects/muc/muc_sw/muc_sw_manual.html
  = http://en.wikipedia.org/wiki/Named-entity_recognition
  = http://www.cicling.org/2009/RCS-41/047-058.pdf
  = http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/PrecisionRecallEvaluation.html
  = http://alias-i.com/lingpipe/docs/api/com/aliasi/classify/ScoredPrecisionRecallEvaluation.html
  = http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html
  = http://www.itl.nist.gov/iad/mig/publications/proceedings/darpa99/html/dir10/dir10.htm
  = http://en.wikipedia.org/wiki/Sensitivity_and_specificity
  = http://repository.dlsi.ua.es/251/1/pdf/869_paper.pdf
    bibliography?
  = http://www.researchgate.net/publication/8048485_Agreement_the_f-measure_and_reliability_in_information_retrieval
  = http://nlp.stanford.edu/IR-book/pdf/08eval.pdf
  = http://www.cnts.ua.ac.be/~vincent/pdf/microaverage.pdf
  = http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1090460/
  = http://blogs.sas.com/content/iml/2011/07/29/computing-an-roc-curve-from-basic-principles/
  = http://gim.unmc.edu/dxtests/ROC1.htm
  = http://www.r-bloggers.com/roc-curves-and-classification/
  = http://weka.wikispaces.com/ROC+curves
  = http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0092209
  = http://www.vlfeat.org/overview/plots-rank.html
  = https://www.creighton.edu/fileadmin/user/HSL/docs/ref/Searching_-_Recall_Precision.pdf
  = http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html
  = http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html
  = http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_DavisG06.pdf
  = http://en.wikipedia.org/wiki/Precision_and_recall
  = http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html
  = http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve
  = http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score
  = http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html
  = https://gate.ac.uk/sale/tao/splitch10.html#x14-27200010 

MISC NOTES:
= algorithm for minimum cost assignment, useful for minimum loss coreference cluster 
  assignments: Munkres/Hungarian algorithm, see http://en.wikipedia.org/wiki/Hungarian_algorithm
  for perl: https://metacpan.org/pod/Algorithm::Munkres 
  explanation and c code: http://csclab.murraystate.edu/bob.pilgrim/445/munkres.html
  explanation/tutorial: http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=hungarianAlgorithm
  implementation/java: https://github.com/KevinStern/software-and-algorithms/blob/master/src/main/java/blogspot/software_and_algorithms/stern_library/optimization/HungarianAlgorithm.java
  implementation/java: https://code.google.com/p/robotutils/source/browse/trunk/src/robotutils/planning/KuhnMunkres.java
